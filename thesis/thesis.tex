%% Thesis template conforming to Williams College rules.
% Thanks to Ben Wood '08 and other contributors.
%
\documentclass[twoside]{report}

\usepackage[top=1.0in, bottom=1in, left=1.5in, right=1in, includehead]{geometry}
\pagestyle{headings}
\usepackage{setspace}

%% Special math fonts and symbols
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}

%% Rotate tables and figures
\usepackage{rotating}

%% Used for TODO items
\usepackage{color}
\usepackage{todonotes}
\newcommand{\todoi}[1]{\todo[inline, color=blue!20]{TODO: {#1}}}

%% used for code listings.
\usepackage{float}

%% Used to replace LaTeX's ugly emptyset with diameter, which looks nicer.
\usepackage{wasysym}

%% Nicely formatted algorithms.
\usepackage{algorithmicx}
\usepackage[chapter]{algorithm}
\usepackage{algpseudocode}

%% Nicely formatted listings.
\usepackage{xcolor}
\usepackage[defaultmono]{droidsansmono}
\usepackage{listings}
\usepackage{etoolbox}

% Display language after listings box
\makeatletter
\AtEndEnvironment{lstlisting}{\xdef\xlang{\lst@language}}
\AfterEndEnvironment{lstlisting}{\begin{flushright}\vspace{-8mm}\texttt{\xlang}\end{flushright}}
\makeatother

\newcommand{\ocamlcommentstyle}{\color{blue}}

% OCaml listing definition: https://gitlab.inria.fr/fpottier/visitors/blob/c54b05ddf255ba50e89943f165b9da7de0ceb3f9/doc/listings-ocaml.tex
\lstdefinelanguage{ocaml}[Objective]{Caml}{
  % Fix errors in the default definition of ocaml.
  deletekeywords={closed,ref},
  morekeywords={initializer},
  % General settings.
  flexiblecolumns=false,
  showstringspaces=false,
  framesep=5pt,
  commentstyle=\ocamlcommentstyle,
  % By default, we use a small font.
  basicstyle=\tt\small,
  numberstyle=\footnotesize,
  % LaTeX escape.
  escapeinside={$}{$},
}
% Shell listing definition: https://tex.stackexchange.com/questions/310335/using-bash-listings-to-bold-variables-and-functions
\lstdefinelanguage{shard}%
  {morekeywords={awk,break,case,cat,cd,continue,do,done,echo,elif,else,%
      env,esac,eval,exec,exit,export,expr,false,fi,for,function,getopts,%
      hash,history,if,in,kill,login,newgrp,nice,nohup,ps,pwd,read,%
      readonly,return,set,sed,shift,test,then,times,trap,true,type,%
      ulimit,umask,unset,until,wait,while,
      cluster},%
   morecomment=[l]\#,%
   morestring=[d]",%
   alsoletter={*"'0123456789.},%
   alsoother={\{\=\}},%
   literate={{=}{{{=}}}1},%
   literate={\$\{}{{{{\bfseries{}\$\{}}}}2,%
   otherkeywords={ [, ], \{, \} }%
  }[keywords,comments,strings]%

\lstset{
    language=ocaml,
    basicstyle=\ttfamily\small,
    aboveskip={1.0\baselineskip},
    belowskip={1.0\baselineskip},
    columns=fixed,
    extendedchars=true,
    breaklines=true,
    tabsize=4,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=lines,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\color[rgb]{0.627,0.126,0.941},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{01,0,0},
    numbers=left,
    numberstyle=\small,
    stepnumber=1,
    numbersep=10pt,
    captionpos=t,
    escapeinside={\%*}{*)}
}

%% More kinds of arrow with stuff
\usepackage{empheq}\usepackage{multicol}
\usepackage{subfigure}

%% urls
\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis body %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page              %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
  $\;$
  \vskip1.5in
  \onehalfspacing
  \begin{center}
    {\LARGE
      Shard - Ad-hoc Reliable Distributed Shell
    }
    \large
    \vskip.25in
    by\\
    Markus Feng\\
    \vskip.125inProfessor Daniel W. Barowy, Advisor\\
    \vskip.125inProfessor Jeannie Albrecht, Advisor\\
    \singlespacing\vskip.375in
    \small
    A thesis submitted in partial fulfillment\\
    of the requirements for the\\
    Degree of Bachelor of Arts with Honors\\
    in Computer Science\\
    \vskip.5in
    Williams College\\
    Williamstown, Massachusetts\\
    \vskip.5in
    \today
    %%\vskip.5in
    %%{\Huge \textbf{DRAFT}}
  \end{center}
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents
\listoffigures
\listoftables
\onehalfspacing

\chapter*{Abstract}

Performing tasks that are distributed among many systems currently fail to achieve each of the goals of being reliable, easy to set up, closely resembling an existing technology, and friendly to users without distributed systems knowledge.
Because distributed systems are inherently unreliable, succeeding in these goals require the system to provide an abstraction for error handling and recovery that account for the many failure cases that may arise.
This thesis project attempts to define a set of error handling semantics that is simple to use, but powerful enough to be applied to a diverse set of use cases.
The project will also involve implementing a shell-based programming language, Shard (standing for Shell - Ad hoc, Reliable, and Distributed), to demonstrate the usefulness of the error handling semantics as defined.
Using this language, anyone with basic shell knowledge should be able to write and run scripts that runs over multiple machines reliably.
This will greatly aid in increasing the accessibility and simplifying the process of running distributed programs in domains including systems administration and scientific computing.


\chapter*{Acknowledgments}

I could not have written this thesis without the help of my advisors, Dan Barowy and Jeannie Albrecht, who have both been invaluable in guiding me along in the process. Special thanks to both Dan and Jeannie for taking me on as a thesis student during their sabbadical leaves, during a difficult time in the world, as we work through the COVID-19 pandemic in all facets of our lives.

\todoi{Expand}

%%%%%%%% Chapters %%%%%%%%%%%%

%% Introduction
\chapter{Introduction}

\section{Motivation}
% \begin{itemize}
%   \item What makes this project unique (will elaborate in background)
%   \item Introduce three pillars (distributed, reliable, easy to use)
% \end{itemize}

The ability to run a program or task over a network is useful for programmers and computer users in many situations, particularly because of increasing popularity in moving computational resources to the cloud or remote datacenters.

Many developments in distributed computing focus on designing and implementing systems that work well in a large-scale, complex, scalable, and highly performant enviornment, with more details about these systems in the \textbf{Distributed Computing} section of the \textbf{Background} chapter.

Frequently, the tradeoff for these systems is that they involve  complicated configuration and handling to sure that they work in the first place, making it often take a disproportionate amount of effort to perform a simple task in a distributed manner.
Examples of configuration needed in these systems include adding IP address whitelists, opening ports, installing the distributed platform on each machine, setting up external dependencies, creating a heavyweight project for each script, and tuning various parameters, just to name a few.

This thesis describes the design and implementation of the Shard programming language, designed to make it easy for any programmer to create and run a distributed program with minimal configuration, regardless of the programmer's background in distributed systems. Shard extends the shell programming language by abstracting away the distributed system and handling any errors that may occur in the running of the program as a result of failures in the distributed system.

We have defined three core pillars that combine to describe the goals of Shard: Distributed, Reliable, and Easy to use. Our claim is that Shard is specifically designed to achieve the goals set by each of the three core pillars, whereas most computing systems only satisfy at most two out of the three pillars.
We will describe each of the core pillars in more detail, including why the core pillar is important in achieving the goals of Shard.

\subsection{Distributed}
% \begin{itemize}
%   \item Why would someone want a distributed program?
%   \item What are some challenges of a distributed program?
%   \item Talk about error handling, segue into reliability
% \end{itemize}

The first core pillar of Shard is that it is distributed.
This means that a Shard program should be able to interact with a set of many computers, some of which may be located across a network boundary.
Shard should be explicity designed in a way to make it easy for parts of a program to be distributed, with syntax-level support for running parts of the code on the distributed system that seamlessly integrate with language features.
A typical Shard program that utilizes the distributed system should not be significantly more complex than a comparable program that performs all of its computations locally.

There are many reasons why a distributed program may be wanted or beneficial.
One example of why a program may want to be distributed is that a program wishes to use resources on other machines across a network, where the remote machine may have more abundance or even exclusive access to certain resources.
These include hardware resources such as CPUs, GPUs, or special purpose hardware; software resources such as the presence of certain operating systems, programs, or libraries; and data resource such as databases and other data stores.

Another example of a program that benefits from being distributed is a program that wishes to perform a certain task on a specific set of machines, such as installing packages, colleting logs, or configuring the system.

Most programming languages are not designed to include distributed computing as a first-class citizen.
Assumptions are made that the program is run entirely on one machine (and sometimes even a single core), and adding distributed features to an existing program may involve rewriting large parts of the original program, including clunky external dependencies, or restricting the program to limited functionality within the programming language.

One reason why distributed computing support is limited among languages is that there are numerous difficulties that arise specifically in distributed systems. Some of these include discovery of remote systems, connecting to the system, data transfer, system state serialization, and concurrency.
% cite from somewhere? unsure
One difficulty in particular is the problem of error handling, as a distributed system is inherently unreliable. This will be addressed in more detail in the description of the next core pillar, reliability.

A notable programming language that includes specific fault-tolerant distributed systems support is Erlang \cite{armstrong2010erlang}. Erlang uses the approach of message passing between processes as the method of inter-process communication, in which processes on a distributed network can participate in a single appliccation using this message passing approach, taclking both the issues of concurrency and parallelism.
However, this approach requires programmers to learn a completely new style of programming when workin with distributed programs, and thus is not used in Shard.

Instead, Shard builds on top of the shell programming language, specifically the POSIX shell, which is described in more detail in the \textbf{Command-line Shells} section of the \textbf{Background} chapter. Many extensions of the POSIX shell are widely used by system administrators, programmers, and other computer users.

Notably, the POSIX shell does not contain special support for distributed computing. Shard is designed to be a superset of the POSIX shell, with additions to its syntax, semantics, and built-in functions that result in direct support for distributed computation, allowing Shard to satisfy the core pillar of being distributed.

\subsection{Reliable}
% \begin{itemize}
%   \item Error handling
%   \item Why is reliability important?
%   \item Segue - how does it tie into being easy to use
% \end{itemize}

The second core pillar of Shard is that it should be reliable.
This means that Shard should make a best-effort attempt to complete the distributed computing task that is assigned to it.
Shard should be able to internally handle any errors encountered during the execution of the distributed program, such as a temporary or permanent failure to connect to a host, or loss of data along the way, only exposing the error to the user when the failure cannot be overcome.
This means that using Shard should result in a more reliable exprience than the reliability of the underlying distributed system itself.

In general, this core pillar is much more difficult to satisfy for distributed programs compared to non-distributed programs, because while non-distributed computer programs are often treated as usually reliable in executing an average set of instructions, barring some exceptional circumstance, distributed programs work in the opposite manner, where reliability is assumed not to hold.

The specific reasons why distributed systems are unreliable are explained in more detail in the \textbf{Distributed Computing} section of the \textbf{Background} chapter, but the basic summary is that distributed systems work on top of an ultimately unreliable network, and each machine in the system having a chance of failure means that it is exponentially more likely for the entire system to have one or more failures in total.

This pillar needs to be addressed by Shard in particular because many simple computer systems that satisfy the other two pillars of being distributed and easy to use fail to satisfy this pillar, which causes the system to be much less useful in practice, where failures in the distributed system often do end up occuring.
For example, the examples mentioned in the \textbf{Distributed Command-line Shells} section of the \textbf{Background} chapter generally focus on performing the distributed task on the machines in the distributed cluster, but do not provide special handling to circumstances when one or more of the tasks fail to complete.

Reliability is important because it allows Shard programs to be more predictable and deterministic, characteristics that are desirable for most computer programs. If reliability were not a part of the a platform itself, then every program building on that platform would have to include extra code to cover for any problems that occur, which can end up being inefficient, time-consuming, and error-prone itself, whereas building error-handling mechanisms at the platform-level as Shard does can provide reliability to all programs on that platform without special work.

\subsection{Easy to use}
% \begin{itemize}
%   \item Ease of use
%   \item Ad-hoc
%   \item Low knowledge barrier
%   \item Why is being easy to use important?
%   \item Distinguishes Shard from other projects
% \end{itemize}

The third core pillar of Shard is that it should be easy to use.
This means that Shard should be useful in an ad-hoc situation for any programmer to run a distributed program.
Shard should be easy to use for someone with little to no background knowledge in distributed computing and in Shard itself, so from a language design standpoint, it should be as clear as possible in both reading a Shard program to understand what it does, and in creating a Shard program to complete a certain task.
Shard should be also be easy to use on any machine that is supported along with any cluster of machines to perform the distributed computation on, with minimal amounts of installation and configuration on both the machine starting the Shard program and the machines in the cluster, and this minimal configuration and installation should also not require much background knowledge to perform.
It should be fast and easy to get from the point of Shard being installed to running the program on Shard, particularly for somone who has already used Shard before.

There are clear benefits to a computer system for being easy to use.
From the standpoint of having a low knowledge barrier, ease of use can mean that the system is useful to a larger audience of users, since the barrier of learning a new tool or its prerequisite background knowledge may stop larger numbers of people from trying it out in the first place, resulting in the system only being useful for a small niche of trained experts, even if the ultimate benefit of using the system is greater than the learning cost.
In addition, Shard being easy to use may mean that it is the better tool in ad-hoc situations, where the goal is to solve a problem of running a distributed program using some combination of the least effort and the least time, which can give it a significant edge over more powerful but heavyweight distributed systems, where time is wasted setting up the system or writing the distribued program.

The core pillar of being easy to use is what separates Shard from many traditional distributed systems that will be described in the \textbf{Distributed Computing} section of the \textbf{Background} chapter, and which do follow the other two core pillars of being distributed and reliable. For many of those systems, it is plausible that the time it takes to get the system up and running is many times longer than the time it takes to install Shard, write the Shard program, run the Shard program, and complete the task.

\section{Summary}
% \begin{itemize}
%   \item What my project is
%   \item What I've done
%   \item How I've done it
%   \item Measurement of success
% \end{itemize}
% \begin{itemize}
%   \item Explain the layout of the rest of my thesis
% \end{itemize}

The Shard project's goal is to design and implement a distributed shell programming language that accomplishes the goals outlined by the three core pillars of being distributed, reliable, and easy to use.
The work and evaluation of the Shard project will be described in the remainder of the thesis, separated into the following parts:

\textbf{Chapter 2} describes the background of related computer systems, including other non-distributed and distributed command-line shells, and other distributed systems.

\textbf{Chapter 3} describes the design of the Shard, introducing the concept of application classes and providing details about the design of both the Shard programming language and the protocol used in the communication of the distributed system.

\textbf{Chapter 4} describes the implementation of Shard, separated into the, shell, language, protocol, and application class implementations.

\textbf{Chapter 5} describes the evaluation of the Shard distributed system and how well it fits the goals of the three core pillars as described in this chapter.

\textbf{Chapter 6} provides a conclusion to this thesis, summarizing the contributions and providing potential directions for future work.


%% Background
\chapter{Background}

This thesis describes a set of error handling semantics for a distributed command-line shell, along with Shard, an implementation of a distributed shell with those semantics.
This chapter starts with describing the background behind command-line shells, focusing in particular on their relationship with the UNIX operating system and the specification of the POSIX shell.
After that, this chapter moves on providing an overview of distributed computing, with a deeper look at their fault tolerance properties that will guide the error handling semantics in this thesis, and several examples of distributed systems, including Hadoop, Ganglia, and Plush.
Next, this chapter will talk about existing examples of distributed command line shells, including their features, implementations, and weaknesses.
Finally, this chapter closes out with a summary of existing shells, distributed systems, and distributed shells, along with how the Shard shell builds upon these existing systems.

\section{Command-line Shells}
\subsection{UNIX Shell}
UNIX is an operating system designed at Bell Labs, with the first versions appearing around 1969-1970 \cite{10.1145/361011.361061}.
UNIX was an influence in many operating systems in use today, with its original form evolving into various BSD distributions (NetBSD, FreeBSD, OpenBSD), and the GNU operating system, which is the basis of Linux distributions today, was designed to be UNIX compatible \cite{bretthauer2001open}.

One of the standard ways of interacting with the UNIX operating system is through a command-line shell \cite{10.1145/361011.361061}.
The operation of the shell involves reading in lines of text typed by the user, and performing a command based on the input text.
The most basic command consists of a space-separated list of strings, with the first string being the name of the command, and the remaining strings being the arguments for the command.
The shell searches the file system for the appropriate program to run, based on the name of the command, and feeds the arguments as input to the program.
By default, the standard output of the program is displayed back to the user by the visual interface of the shell program, but indirections can modify this by transferring the output to a file or another program.
When execution completes, control is given back to the user, who can run another command line command on the shell.
Because the command line is a program itself, the shell can run itself as a program with the input being a file containing shell commands.
The command-line shell is useful in a wide variety of contexts, in being one of the easiest ways to programmatically interface with the operating system itself, and many operations on computers are able to be performed or even require using shell \cite{10.1145/3371111}.

\subsection{POSIX Shell}
POSIX is a family of IEEE standards originating in the early 1980s that specify a portable operating system interface based on UNIX \cite{10.1145/210308.210315}.
Contained within the POSIX specification is a specification of the POSIX shell, which describes the interface that the shell command language should conform to \cite{posix2017}.
Because each operating system that conform to the POSIX interface must implement a POSIX shell, there have been a wide variety implementations of POSIX shells on numerous platforms, many of which have additional features that extend the capabilities of the shell.
The POSIX shell is popular across computing platforms, including even platforms that are not designed to be POSIX compatible, though notably a majority of POSIX shells do not fully conform to the POSIX specification \cite{10.1145/3371111}.

\section{Distributed Computing}

A distributed computing system, or a distributed system, is a computer system that consists of multiple processors one one or more machines that work together but do not directly share memory \cite{10.1145/72551.72552}.
This thesis in particular focuses on multicomputer distributed systems that communicate over a network that is not guaranteed to be reliable.
As such, there may be errors or faults that occur during the operation of the system.

\subsection{Fault Tolerance in Distributed Systems}
There has been significant work in the field of fault tolerant distributed systems, as one important goal in computer systems is to be dependable, which comes with significant challenges when in a distributed environment, compared to a program running a single process or machine \cite{10.1145/311531.311532}.

% Safety/Liveliness
The two classes of fault tolertance properties in distributed programs are properties of safety and liveliness \cite{1702415}.
Safety properties ensure that the state of the distributed system must not be some state, while liveliness properties ensure that the state of the distributed system must eventually be in some state.
For example, a safety property of a distributed system may be that one part of a distributed program is not executed until a different part is complete, while a liveliness property of a distributed system may be that the program eventually provides a result, even if one or more machines are stuck in a loop during execution.

% Masking/Fail-safe/Nonmasking
When designating a system to be fault tolerant, it is important to specify which types of faults that the system is tolerant against, and what types of fault tolerant properties are satisfied by the system \cite{10.1145/311531.311532}.
In the strongest form, a system with masking fault tolerence is able to satisfy both safety and liveliness properties against a given set of faults.
Slightly weaker is fail-safe fault tolerance, which ensures safety but does not make liveliness guarantees, capturing the notion that certain illegal states will never appear.
Safety is important in many situations, such as making sure that a missile does not launch in case of individual component failures.
On the other hand, nonmasking fault tolerence, which ensures liveliness but does not make safety guarantees, captures the notion of a system always making progress towards completion, despite the possibility of some amount of incorrect behavior with the program.
Traditionally less focus has been placed on systems in ensuring liveliness compared to ensuring safety, as liveliness issues can be resolved in many cases by manually stopping the system, whereas safety issues can lead to large amounts of damage.

% Potential sections:
% Crash/Fail-stop/Byzantine
% Redundancy
% Error detection and correction

\subsection{Examples}
Fault tolerant distributed systems exist in many forms, with widespread practical usage.
Typically, these systems target solving a specific type of distributed problem, with differences based on the priorities and goals of the individual system.

\subsubsection{MapReduce/Hadoop}
MapReduce is a distributed computation framework developed by Google to handle and process large amounts of data \cite{dean2004mapreduce}.
The data processing is done by splitting up the input into key-value pairs, applying a function in a distributed fashion on each of those value pairs, using a reduce function to combine the pairs that share a key.
In the implementation used by Google in 2004, a typical MapReduce cluster can contain hundreds or thousands of machines.
The problem of making data available to other machines is solved through the use of a distributed file system formed by the disks located at each of the machines in the cluster, which partitions the data storage to individual machines.
Because the data is stored on a particular machine, that same machine can be in charge of performing the necessary map and reduce computations.

Apache Hadoop is an open source distributed programming framework that can be used to run MapReduce-style computation on large datasets \cite{Hadoop}.
Hadoop is designed to target the use case of data processing with extremely large data sets that cannot be handled with ordinary programming techniques that depend on all of the data to be available on one machine and the work being done on a single process.
Hadoop uses a distributed filesystem in a similar manner to Google's MapReduce implementation, with explicit goals of being able to handle faults such as hardware failure, using techniques including replication to make sure that the data is stored reliably across the cluster \cite{borthakur2007hadoop}.
The fault tolerant properties of Hadoop makes it useful in production usage, where errors are to be expected to occur when dealing with large clusters of machines.

\subsubsection{Ganglia/GEXEC}
Ganglia is a monitoring system for scalable high-performance distributed programs \cite{MASSIE2004817}.
One of the central goals of Ganglia is robustness.
Ganglia is designed to monitor failures and continue to operate in the presence of these failures, as individual failures are common in large-scale systems with high resource demands and utilization.
Types of failures as identified by Ganglia include computational, I/O, and network failures on individual machines, along with bottlenecks on a larger scale.

As a monitoring system, it is possible to use the information provided by Ganglia in an application or an application framework.
The utility GEXEC allows executing jobs of machines on a cluster, while providing data and signal forwarding on jobs between the machines \cite{gexec}.
GEXEC supports a mode of operation that utilizes Ganglia, which takes advantage of Ganglia's failure monitoring and handling features to ensure that the system is robust even on large distributed clusters.

\subsubsection{Plush}
Plush is a unified framework for managing various classes of programs to be executed in a wide variety of distributed environments \cite{10.5555/1349426.1349441}.
Like the other distributed system frameworks mentioned above, Plush has an emphasis on detection and recovery from failures through monitoring the state of the system.
Plush acts as a platform for building applications on, providing a set of abstractions and configurations to allow error recovery in a user-specified manner, designed in a fashion to be applicable to a wide range of different types of distributed programs.

\section{Distributed Command-line Shells}
The concept of command-line shells and distributed computing can be combined to form a distributed command-line shell.
A distributed shell is a shell that supports command-line shell features over a distributed cluster of machines.

\subsection{Examples}
The concept of distributed shells has been visited in the past in various forms.
However, none of these shells fully realize the goals of the Shard shell.
In particular, unlike for many of the distributed systems mentioned above, the issue of distributed error handling and recovery is not well addressed by any of the solutions below.

\subsubsection{SSH}
One method of making an ad-hoc distributed shell is to invoke the SSH secure shell utility multiple times.
The SSH utility allows users to connect to remote machines over TCP and access the command line shell on those machines \cite{rfc4251}.
Given a command to run and a list of remote hosts, the shell can iterate over the list, invoke SSH with that remote host as an argument, run the command on the remote host, and finally collect the results back on the local machine.
Notably, this method is that it does not handle error recovery well, or guarantee reliable execution in the event of failures.

\subsubsection{DSH (Dancer's Shell)}
The DSH (Dancer's Shell) Unix program allows the user to send a shell command to all machines in a cluster defined by the user \cite{dshdancer}.
DSH uses an underlying remote execution protocol, such as SSH, for executing shell programs on remote hosts.
The command invocation occurs in a hierarchial fashion, with each node sending the command to some amount of child nodes until the program is executed on all nodes in the cluster.
Notably, for the DSH program, there does not exist an emphasis of reliable execution and error recovery from failures that may arise from the distributed nature of the program.
Other similar implementations of the DSH program exist in various forms, some of them implemented with a different language or work on different platforms.

\subsubsection{Distsh}
Distsh (Distributed Command Line Interface) is an implementation of a generic distributed shell environment, with steps taken to make accessing a remote machine just like accessing a local one \cite{distsh}.
The Distsh shell language only specifies semantics for connecting to a single remote machine at a time, rather than having the unit remote machine being a cluster as is the case in DSH.
Similar to DSH, there does not exist error handling semantics to allow reliable execution and error recovery.
The paper specifies that the implementation currently only works when the machine to access is localhost, and there does not appear to have been visible progress on the project since the paper was made public.

\section{Summary}
Command-line shells are an important and useful interface for interacting with a computer.
Therefore, it makes sense to be able to use the command-line shell to interact with many computers simultaneously in a distributed manner.
However, distributed systems come with existing challenges, particularly because failures are much more likely when there are more avenues of failure and more machines that can fail.
While many existing distributed systems have built-in error handling and recovery mechanims, prior solutions for extending the shell to a cluster of distributed systems do not sufficiently take into account these same errors.
This thesis aims improve upon existing solutions by unifying the reliability built in to distributed systems with simplicity and ease of use of the command-line shell interface.

\chapter{Design}

In this chapter, we discuss the design of the Shard distributed system, which includes the Shard programming language, the Shard protocol, and the application classes supported by Shard.

\section{Overview}
\begin{itemize}
  \item High level design
        \begin{itemize}
          \item Satisfying the three pillars (ease of use, distributed, reliable)
          \item Use language to solve problem
        \end{itemize}
\end{itemize}

\section{Application classes}

\begin{itemize}
  \item Solves the problem of: reliability/error handling semantics
  \item Overall error model (or maybe keep it specific to the application classes)
  \item Describe the existing application classes
\end{itemize}

\subsection{Application class: \texttt{single\_command}}

\begin{itemize}
  \item Application features
  \item Error handling
  \item Give example of when to use
\end{itemize}

\subsection{Application class: \texttt{data\_parallel}}

\section{Language design}

\begin{itemize}
  \item Solves the problem of: ease of use/familiarity
  \item Describe the programming language
  \item Based on the POSIX shell standard
  \item Simple new syntax/semantics for distributed work
        \begin{itemize}
          \item Cluster management (builtin)
          \item Application classes/error handling semantics support
          \item Run on remote host (@@)
        \end{itemize}
\end{itemize}

\subsection{Example programs}

\begin{itemize}
  \item Give examples and explain in detail
  \item Example program: Hello world
  \item Example program: Memory usage in cluster
  \item Example program: Word count in large file
\end{itemize}

\section{Protocol design}

\begin{itemize}
  \item Solves the problem of: distributed
  \item Describe the distributed protocol
  \item Note that this is about the design of the protocol, rather than the implementation details
\end{itemize}

\chapter{Implementation}

In this chapter, we discuss the implementation of the Shard distributed system, which includes the implementation of the shell, the language, the protocol, and the application classes.

\section{Overview}

Shard is implemented in the OCaml programming language, as a single project that includes the shell, language, protocol, and application classes components of Shard, along with code that combines these components into a single application.
OCaml was chosen for its conveinence in programming language implementation, high level of safety, direct access to UNIX system calls with a wrapper library, reasonable performance, and programmer familiarity of the language.

When built, Shard is an executable invocable on the command line that can be used in a similar fashion as the executable other shell programs, by passing it a file containing a shard program (typically with the \texttt{.shard} extension), or by passing in no arguments to start Shard in interactive mode.
Interactive mode allows Shard to function as a Read-Eval-Print-Loop, meaning that the user can input any Shard command followed by an \texttt{Enter} key press to execute it, giving back control to the user when the command execution completes.
Like for some other shells, Shard allows the user to use the keyboard shortcut \texttt{Ctrl-C} to interrupt the current executing Shard command, and the keyboard shortcut \texttt{Ctrl-\\} to immediately exit Shard.
Specific usage details can be found in the documentation of the project.

Currently, Shard is specifically tested to support Ubuntu Linux version 20.04.2 LTS, but it should work with zero or few modifications on other Linux systems, depending on the requirements of the library dependencies of Shard.

\subsection{Dependencies}
The Shard project depends on various other OCaml libraries and OCaml wrappers for non-OCaml libraries for important pieces of existing funcionality.

The entire project is written on top of Jane Street's open source libraries Core, Async, and Sexplib, along with the OCaml parser combinator library Angstrom and an OCaml wrapper for the SSH library libssh.

Core is an alternative standard library for OCaml with many more features, including additional data structures and extensions to existing standard library modules \cite{ocamlcore}.
Core also provides full UNIX support in its \texttt{Core.Unix} module, including API wrappers for many of the system calls found on Linux and other UNIX machines.

Core depends on the Jane Street library Sexplib, which contains functionality for working with S-expressions, a human redable data representation format commonly used by the LISP family of languages \cite{mccarthy1960recursive}. Sexplib allows automatic generation of functions to convert OCaml objects to and from S-expressions, and is used extensively to convert objects for serialization purposes.

Async provides promise-based asynchronous code support for OCaml, and includes its own \texttt{Async.Unix} that builds on top of \texttt{Core.Unix} to allow system calls and I/O operations to work asynchronously on top of an event queue, while still working under the OCaml language's single core restriction \cite{ocamlasync}.

Shard's language parser depends on the OCaml parser combinator library Angstrom, which is designed to be an easy-to-use parser combinator library specifically for use with OCaml. \cite{ocamlangstrom}.
Shard uses a forked version of Angstrom that adds a few more combinators to Angstrom's long list in order to support specific features of the shell grammar that would otherwise be more difficult to implement with Angstrom's existing interface.

Shard's protocol depends on the SSH library libssh for all of its network communication \cite{libssh}.
Libssh is a widely used SSH library implemented in C that includes compatibility with all of the SSH communication features needed by Shard.
Because libssh is written in C, Shard uses an OCaml wrapper for libssh, mllibssh \cite{mllibssh}.
Due to issues with mllibssh not being well maintained, the version of mllibssh used in Shard is forked and contains modifications including bugfixes and improvements to the wrapper code.

\section{Architecture}

% \begin{itemize}
%   \item Split up into 4 parts: Shell, language, protocol, application class
%   \item Basic summary of each part, its purpose, and how they connect together
% \end{itemize}

The Shard project is split up into four parts: shell, language, protocol, and application classes. The split is informal, as all four parts belong to the same compilation unit, and reside in the same directory.
Each of the sections of the architecture are designed in a way to be easily extensible, in that more functionality can be added without significantly changes to existing code.

The shell is the part of Shard connects the program and the interactive mode to the remaining parts of Shard. This is where the entry point of a Shard program is.

The language is the part of Shard that provides the Shard programming language features, including a recursive descent parser and a tree-walk interpreter. The interpreter also communicates with the protocol and the application classes parts of Shard as needed by the command that it is currently executing, in additional to the operating system for a variety of shell features.

The protocol is the part of Shard that provides all of the distributed systems functionality, including maintaining network connections to remote machines, spawning helper processes, and error detection/handling.

The application class is the part of Shard that include the interfaces of the application class subsystem and the implementation of the specific application classes that are currently supported, which are the \texttt{single\_application} and \texttt{data\_parallel} application classes. Application classes are described in more detail in the \textbf{Application Classes} section of the \textbf{Design} chapter.

In addition to these specific parts of the Shard project, there are also modules for common utility functions, unit testing, and command-line interfacing.

\section{Shell implementation}

% \begin{itemize}
%   \item In depth writing about how the shell is implemented
%   \item OCaml, directly on top of Unix primitives (rather than on an existing shell)
%   \item Invokable with command line
%   \item Interactive and non-interactive mode
%   \item Most features are written to work locally, and separated so that language/protocol/application class implementations are abstracted
% \end{itemize}

The Shard shell is implemented in OCaml, directly on top of UNIX primitives from the OCaml libraries Core and Async.
An alternative solution would be to take an existing Shell application and modify it to support the Shard programming language and platform, which would have the benefit of letting Shard support many additional Shell features for free, such as tab-compleition, Shell history, and custom colors, depending on the feature set of the Shell it extended.
However, using a existing shell would also offer much less flexibility in terms of making changes that radically transform the behavior of the shell, such as making remote execution functionality seamlessly work alongside local functions in the shell.

When the shell is invoked in interactive mode, it works as a Read-Eval-Print-Loop (REPL). The REPL reads a line from the standard input (stdin) for a Shard command or program. The shell sends the input to the parser, which results in either an abstract syntax tree (AST) corresponding to the input, an error in parsing, or an incomplete parse.
In the case of a successful parse of an AST, the resulting AST is evaluated by the interpreter, and any output of the command is forwarded to the standard output (stdout) or standard error (stderr), before returning to REPL to wait for the input of a a new command.
In the case of the incomplete parse, another line is read from standard input and added to the original input to try parsing again.
In the case of a parse error, the error is printed back to the standard error (stderr), and the REPL waits for further input for a new command.
The text below showcases an example of the output when running Shard interactively:

\begin{lstlisting}[language=]
$ echo "Hello, world!"
Hello, world!
$ hostname@@machine1
remotename
\end{lstlisting}
\todoi{Potentially make these code snippets into proper figures with captions?}

Similarly, when the shell is invoked in non-interactive mode, the shell takes the input file or string to the parser, which results in either an AST corresponding to the input program, or an error if the input fails to parse, in which case the error is printed to stderr.

The shell can be invoked with the \texttt{-s} command line flag to indicate that the input is an S-expression that represents an AST object rather than a string in the Shard language grammar. In this case, the step of parsing with an AST is bypassed, and the AST is directly evaluated by the interpreter, behaving identically to if it were parsed from a program in the language.

The shell also uses the concept of jobs to support the keyboard interrupt feature, which allows the user to cancel the currently running task.
Shard uses a signal handler to catch the \texttt{SIGINT} interrupt signal when \texttt{Ctrl-C} is pressed during a running task.
Rather than exiting Shard itself, instead, causes the Shard evaluation to stop, as each evaluation function checks to make sure that the job is active before performing the evaluation.
If an external command is currently being executed, the signal is passed to that command, and if a remote command is currently being executed, further processes is stopped.

\section{Language implementation}

% \begin{itemize}
%   \item Implemented by referring to POSIX shell specification
%   \item Parse: Use recursive parser combinator library Angstrom
%   \item Interpret: Interpret parsed objects in functions directly, with support for custom features needed by Shard
%   \item Some specification features are not yet supported, but there is no reason why support cannot be added if needed
% \end{itemize}

The Shard programming language implementation is split up into two main parts - the recursive descent parser and the tree-walk interpreter.

\subsection{Shard parser}

The parser is implemented using the parser combinator library Angstrom by following the grammar section in the POSIX shell specification, and extending it with the additional syntax found in the \textbf{Language design} section of the \textbf{Design} chapter \cite{ocamlangstrom} \cite{posix2017}.

A successful parse transforms a \texttt{string} input to a \texttt{Ast.t} variant as output. The \texttt{Ast.t} variant is used to represent a Shard abstract syntrax tree, which is also used in the interpreter when walking through the AST.

The OCaml variant type can be used very effectively to represent recursive structures with complex definitions, along with retrieving their data in a type-safe manner using pattern matching, which is one of the significant benefits to use OCaml for implementing programming languages \cite{realworldocaml}.

In addition to allowing parsing of full strings that are defined by the Shard grammar, the parser also uses an Angstrom feature to support partial parsing, which uses a piece of state to describe whether a parse is complete (the input is a string in the grammar), incomplete (there exists some string that completes parse), or failed (there is no string that completes the parse).
This allows the user to enter a new line after partially inputting their command in the interactive shell, and have the shell prompt for more input, rather than rejecting the input.
Note that inputting an empty line will force the parse to attempt to complete, which will in this case indicate that the parse failed.

For example, inputting the Shard command gives a prompt of \texttt{>} to continue with the command, as the parser sees the unmatched open parenthases.
\begin{lstlisting}[language=shard]
$ (echo
>
\end{lstlisting}

This command can be completed by adding some input and closing the parenthases.
\begin{lstlisting}[language=shard]
$ (echo
>  hello)
hello
$
\end{lstlisting}

One notable distinction between Shard and other shell parsers is that nested shell environments such as subshells in Shard are parsed together with the entire command, rather than as needed.
In the Bash shell, for example, if a subshell is defined in a branch of a boolean expression, and there is a syntax error in the subshell, the result would only be a syntax error if the branch gets evaluated.
On the other hand, in Shard, the parser would fail to parse the entire expression due to failing to parse the subshell.

As such, the following shell program would result in a syntax error in Shard, but not Bash, because the right-hand-side of the boolean expression is never evaluated:
\begin{lstlisting}[language=shard]
false && $(|)
\end{lstlisting}

\todoi{Do I need to cite subshell behavior for other shells?}

The parser's interface exposes the functions that are called by the other parts of the program to convert between Shard programs as strings to its AST. The following OCaml interface describes the input and output types of the parser:

\begin{lstlisting}
(* Excerpt from ast.mli *)
(* AST type definition omitted *)
type t
(* ... *)
[@@deriving sexp]

val parse : string -> t Or_error.t

val parse_partial
  :  ?state:t Angstrom_extended.Buffered.state
  -> string
  -> t Angstrom_extended.Buffered.state
\end{lstlisting}

As we can see from this interface, the \texttt{parse} function is performs a regular parse of any input string, and emits a \texttt{Ast.t Or\_error.t} variant, which either holds the value of an \texttt{Ast.t} variant (the representation of the AST itself), indicating a successful parse, or an \texttt{Error.t} record, indicating a failed one.

In addition to the \texttt{parse} function, the interface also exposes the \texttt{parse\_partial} function, which corresponds to the partial parsing behavior as described above, taking in an optional partial parsing state and the input string, and spitting out a new partial parsing state corresponding to the status of the partial parse.

Furthermore, from the definition of \texttt{Ast.t}, we can see that it includes the OCaml ppx (preprocessing instruction) \texttt{[@@deriving sexp]}, which autogenerates functions that allow conversion of the \texttt{Ast.t} variant to and from the \texttt{Sexp.t} variant representing an s-expression that losslessly encodes the AST and can be transmitted as a string.

\subsection{Shard interpreter}

The Shard interpreter is a tree-walk interpreter that crawls through the abstract syntax tree and evaluates it node-by-node.

The interpreter is implemented with a set of mutually recursive functions, with each function taking the name \texttt{eval\_PART}, where \texttt{PART} indicates the node on the AST being evaluated by that function.
Evaluating an AST node involves calling the \texttt{eval\_PART} functions corresponding to its children nodes, if any, and then performing an operation based on the type of node being evaluated.

% A concrete example of a type of branch node of the AST is the type \texttt{Ast.pipeline}, with the following definition
% \begin{lstlisting}
% type t = (* ... *)
% and pipeline = bang option * command list
% and bang = Bang
% \end{lstlisting}

% The pipeline represents a chain of commands piped together in Shard, where the entire pipeline can be optionally negated by a bang (\texttt{!}) symbol to change the truthiness of its return code.
% Here is a shell program that finds all of the lines of the input with the character `!':
% \begin{lstlisting}[language=shard]

% cat input.txt | grep "!"

% \end{lstlisting}

% This is represented by a \texttt{Ast.pipeline} made up of the \texttt{None} option}, followed by two \texttt{Ast.command}s.

% The following excerpt from the \texttt{eval\_pipeline} function demonstrates evaluating an \texttt{Ast.pipeline}:

% \begin{lstlisting}
% (* Excerpt from eval.ml *)
% and eval_pipeline pipeline ~eval_args =
%   match pipeline with
%   | [] -> return 0
%   | x :: xs ->
%     (match xs with
%     | [] -> eval_pipeline_part x ~eval_args
%     | _ ->
%       let pipe_read, pipe_write = Pipe_process.pipe () in
%       let deferred_code =
%         eval_pipeline_part x ~eval_args:{ eval_args with Eval_args.stdout = pipe_write }
%       in
%       let new_deferred_code =
%         eval_pipeline xs ~eval_args:{ eval_args with Eval_args.stdin = pipe_read }
%       in
%       let%bind _code = deferred_code in
%       let%bind () = Fd.close pipe_write in
%       let%bind new_code = new_deferred_code in
%       let%bind () = Fd.close pipe_read in
%       return new_code)
% \end{lstlisting}

% Most AST nodes behave as they would in other shell languages, and their implementation is based on following the semantics as defined by the POSIX specification.
% Some notable examples of AST nodes that are treated differently in Shard are \texttt{Ast.subshell} and \texttt{Ast.remote\_command}.

The basic building block of the shell is the simple command, made up of a command name and command arguments, along with any assignments and I/O redirection that are associated with the command.
The command name can either refer to a builtin or an external command.
In the case of a builtin, the functionality is implemented in OCaml itself as a part of \texttt{builtin.ml}.
In the case of an external command, it is executed using the Spawn library, a dependency of the OCaml Core library, behaving in a similar manner to the \texttt{exec} system call, taking in as input the command name, arguments, enviornment, and associated file descriptors for the standard I/O.

During evaluation, a record of the type \texttt{Eval.Eval\_args.t} is passed to each \texttt{eval\_PART} function to represent the input state required for the evaluation to succeed. The definition of the record is shown below:

\begin{lstlisting}
(* Excerpt from eval.ml *)
module Eval_args = struct
  type t =
    { env : Cluster_type.t Env.t
    ; stdin : Fd.t
    ; stdout : Fd.t
    ; stderr : Fd.t
    ; verbose : bool
    }
  [@@deriving fields]
  (* some function definitions omitted *)
end
\end{lstlisting}

The \texttt{Eval\_args.t} is made up of the shell environment (\texttt{Cluster\_type.t Env.t}), file descriptors for the standard I/O of the command being evaluated, and a verbose flag.
By default, the standard I/O corresponds to the standard I/O of the Shard program itself.
This is modified when pipelining and I/O redirecting are introduced.

A pipeline indicates that the output of a command should be passed to the input of the next command with a pipe, as shown in the following example shell program that finds all of the lines of the input with the character `!':
\begin{lstlisting}[language=shard]
cat input.txt | grep "!"
\end{lstlisting}
Originally, this was achieved by manually transferring the data from one program to the next with a virtual pipe-like construct, but this resulted in many practical problems, such as producing the correct behavior when the pipe is closed on either end.

Thus, the piping behavior was changed to use UNIX pipes.
When a pipe operation occurs, a UNIX pipe is created.
The environment's stdin is used as the first command's stdin file descriptor, the environment's stdout is used as the second command's stdout file descriptor, and the two ends of the UNIX pipe are used as the remaining two file descriptors to join the first command's output with the second command's input.
This can be invoked recursively to easily extended to support arbitrarily many commands in a pipeline.
When both programs exit, the pipe must be explicitly closed.
Otherwise, a resource leak can occur, where the Shard runtime stops functioning due to reaching a limit on the number of open file descriptors.

File I/O redirection works very similarly as pipelining, instead the stdio file descriptor with that of a file with the specified filename.
Currently, the Shard interpreter does not support I/O redirection on custom file descriptors other than the standard I/O, but the \texttt{Eval\_args} can be augmented to support the additional redirection capabilities.

A defining feature of Shard is its remote command execution capabilities, where an example of a remote command is shown in the following example command that prints out the host name of a remote machine:

\begin{lstlisting}[language=shard]
hostname@@machine1.cs.williams.edu
\end{lstlisting}

Evaluating a remote command is implemented in a similar fashion to evaluating a subshell.
However, for remote execution, all of the pieces of information needed to evaluate the expression are serialized into s-expression form and sent across the network to a remote instance of the Shard runtime.

If the target of the remote command is a cluster, the application class implementation associated with the cluster will be used for specifics on the implementation of the remote command.
% which exposes the following public API to the interpreter:
% \begin{lstlisting}
% (* Excerpt from application_class.ml *)
% val remote_run
%   :  cluster_id:string
%   -> remote_targets:Remote_target.t list
%   -> setting:string
%   -> program:Sexp.t
%   -> env_image:Env_image.t
%   -> verbose:bool
%   -> stdin:Reader.t
%   -> stdout:Writer.t
%   -> stderr:Writer.t
%   -> unit Deferred.Or_error.t
% \end{lstlisting}
On the other hand, if the target is a host (which is assumed to be the case when the cluster with the given name is not found), the default application class implementation behaving similarly the \texttt{single\_command} application class is used instead.
More details of the remote execution will be specified in the \textbf{Protocol implementation} section.

\todoi{Maybe include a section on asynchronous execution (shell ampersand operator)}

As of the time of writing, some of the features in the POSIX shell specification are not supported by the parser or interpreter of the Shard shell.
However, there is no reason why support for these features cannot be added if needed, with the primary constraint being that the POSIX shell specification is extremely long and contains many features that are not very commonly used by most users in practice.

\section{Protocol implementation}

% \begin{itemize}
%   \item Model: copy executable to each remote machine, then use SSH to communicate between machines
%   \item Support SSH config
%   \item Each remote invocation spawns two processes locally and two processes remotely: justified by easy to separate remote invocations
%   \item Use OCaml Async RPC for interprocess communication between processes on the same machine: strongly typed, serialization supported, etc
%   \item Heavyweight but works well for ad-hoc programming
% \end{itemize}

Many choices can be made in terms of deciding on how to implement the protocol for Shard.
The choices outlined in this section are chosen on based on a variety of criteria, including availability, usability, and ease of implementation, but a different set of choices can result in an equally good or even better implementation of Shard.

\subsection{Network transport}
This implementation uses the Secure Shell (SSH) protocol as a basis for its data transportation over the network.
SSH is a widely supported protocol for accessing a command line shell on a remote machine over a network, with the popular implmentation OpenSSH available on many UNIX platforms \cite{openssh}.
SSH supports security, authentication, and privacy, so those features can be leveraged by Shard and do not have to be built into the Shard protocol implementation from scratch \cite{rfc4251}.

Shard uses the SSH library libssh by leveraging its implementation of the SSH client protocol.
Currently, authentication is handled by using SSH key-pairs to automatically perform the authentication.
For each remote machine in the cluster, the local user should have key-based SSH access to that machine through OpenSSH, by adding the local user's public key to the \texttt{authorized\_keys} of the remote host.
If the user has an OpenSSH configuration file (\texttt{sshd\_config}) at the default location (at \texttt{~/.ssh/}), it is used to allow custom host names and private key names, which can be used for remote targets in the cluster definition.
Otherwise, the user should make sure that the necessary keys are added to their OpenSSH agent for automatic authentication.
This authentication procedure is standard for regular SSH access, and Shard does not need anything extra in order for the SSH transport to work correctly.

\subsection{Data transfer framework}

The Shard protocol involves setting up two connections between four endpoints in order to perform all of the necessary data transfers.
This is implemented by running four additional instances of Shard whenever a remote command is executed on a remote host, each on a separate process.
Separate processes are used for each instance specifically to make it easier to support many simultanous remote connections while being able to recover from failures by killing processes and spinning up new ones as needed.
Subsequent remote commands on the same remote host attempt to reuse the same four instances of Shard, as creating new instances is a relatively expensive operation.

These four instances each correspond to a role as specified by design of the Shard protocol, as specified in the \textbf{Data transfer design} section of the \textbf{Design} chapter.
The four roles are Local Sender, Local Receiver, Remote Sender, and Remote Receiver, with the two local processes being on the local machine, and the two remote processes being on the remote machine.
The procedure for executing a remote command is shown in Figure \ref{fig:protocol_impl}, and will be explained in more defail in the \textbf{Data transfer setup} and \textbf{Remote program execution flow} sections that follow.

\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.5]{img/shard_protocol_impl.png}
    \caption{The procedure for executing a remote command with the Shard protocol}
    \label{fig:protocol_impl}
  \end{center}
\end{figure}

% \todoi{Actually write this section of the design chapter to explain these}
% We explain each of the four ``roles'' in the Design section

The processes on the same machine communicate with each other using remote procedure calls (RPC) from the RPC library Async\_rpc, which is a part of the OCaml Async library.
This library allows the creation of type-safe RPCs that automatically serialize queries and responses with the help of OCaml annotations, making it very easy to set up an RPC service for all of the remote calling functionality necessary.
The RPC servers are accessed by invoking an RPC dispatch call on a particular port on the machine, which is randomly generated and given to the program when the RPC server is created.
Although the RPC servers are not particularly lightweight, only two are used on the local machine for each remote machine connection, so this does not cause a usability problem from a performance standpoint.

\subsection{Data transfer setup}

The first step of the setup is making a copy of the local Shard executable to run the Local Sender and Local Receiver on.
Note that all four processes use an identical executable to the original Shard executable, but with a command argument indicating which role the processes takes on.
These command line arguments should only be used by remote execution mechanism for Shard, and not used by the end user.

\begin{figure}[h]
  \begin{center}
    \begin{tabular}{|l|l|l|l|}
      \hline
      Process role    & Connects to     & Argument & Additional arguments               \\ \hline
      Local Sender    & Remote Receiver & -Rls     &                                    \\ \hline
      Local Receiver  & Remote Sender   & -Rlr     &                                    \\ \hline
      Remote Sender   & Local Receiver  & -Rrs     & Port of Remote Receiver RPC server \\ \hline
      Remote Receiver & Local Sender    & -Rrr     &                                    \\ \hline
    \end{tabular}
    \caption{The command line arguments for running the executable as each of the four process roles}
  \end{center}
\end{figure}


The local copy of Shard is made with a copy attempt with the source being the currently running executable, and the destination being a directory known as the Shard cache, with a fixed path (that may change in future versions of Shard).
A copy attempt means checking whether a Shard executable exists at the destination location, and comparing a cryptographic hash value of the destination file's contents with the source file to make sure that if a file exists, it is the exact same version.
If either no file exists or the destination file has a different hash from the source file, a copy is made.
Once the local copy of Shard is successfully created, the Local Sender and Local Receiver processes are spawned.
Upon spawning, RPC servers are created for the Local Sender and Local Receiver, and their ports are printed to standard output, so the parent processes is able to make dispatch RPC calls to the two processes.

The second step of the Shard protocol is to distribute a copy of the Shard executable to the remote machine.
This is done by dispatching an \texttt{Open} RPC query to the Local Sender, which has the following record type as its query type:
\begin{lstlisting}
(* Excerpt from rpc_local_sender.ml *)
module Open_query = struct
  type t =
    { host : string
    ; port : int option
    }
  [@@deriving bin_io, fields]
end
\end{lstlisting}
The annotation \texttt{[@@deriving]} indicates that the compiler will autogenerate code for the record type, where \texttt{[@@deriving bin\_io]} autogenerates functions to serialize the record in binary form for the RPC, and \texttt{[@@deriving fields]} autogenerates getter functions for accessing individual fields.
For this query, the Local Sender uses the SSH shell copy (SCP) functionality to make a copy attempt of the Shard executable to the remote machine.
In this case, it is particularly imporant to check if the Shard executable already exists in the remote Shard cache, as transferring the executable every time can be very expensive in terms of bandwidth and time.

Once the Shard executable is copied to the remote machine, the next step is to spawn the Remote Receiver processes on the remote machine from the Local Sender via SSH's shell execution mechanism.
The Remote Receiver also creates and RPC server that is used to communicate with the Remote Sender at a later point.
This Remote Receiver RPC server's port is passed back to the Local Sender, and the passed back to the parent process as the query response to the \texttt{Open} query.

Now, the Local Receiver spawns the Remote Receiver sender processes on the remote machine, once again via SSH's shell execution mechanism, and passes in the port of the Remote Receiver RPC server, which allows the Remote Receiver to be connected to the Remote Sender with a pipe on top of the RPC service.

Finally, the system is ready to accept a remote program execution.
Note that future remote program executions on the same remote host can reuse the same four processes, so the earlier parts are only necessary when starting a remote program execution on a new host, or if the processes fail and re-execution is scheduled on a new set of processes.

\subsection{Remote program execution flow}

When a remote program is executed, the first step involves dispatching a \texttt{Header} RPC to the Local Sender, which has the following record type as its query type:
\begin{lstlisting}
(* Excerpt from rpc_common.ml *)
module Header = struct
  type t =
    { program : Sexp.t
    ; env_image : Env.Image.t
    }
  [@@deriving sexp, bin_io, fields]
end
\end{lstlisting}
As shown here, the query for starting the remote program includes the program's AST representation \texttt{Ast.t} in s-expression form, as a \texttt{Sexp.t}, and a serializable image of the environment \texttt{Env.t}, which includes parts of the shell execution environment for this specific program's execution, such as assignments and cluster definitions.
In addition to the \texttt{[@@deriving bin\_io]} and \texttt{[@@deriving fields]} mentioned in the previous section, this record type also contains the \texttt{[@@deriving sexp]} annotation to autogenerate functions for serialzing and deserializing the record as an s-expression, which is useful when sending the query as an s-expression string over SSH.
% reasoning for remote sender/receiver will be explained in the design section
The Local Sender uses SSH to send the entire query in s-expression form to the Remote Receiver, which passes the query on to the Remote Sender.
The Remote Sender Shard processes is the one that actually performs the program execution, using the program and environment as specified by the query.
In addition the the \texttt{Header.t} query, the Local Sender also generates a unique ID for this program execution that is sent to the Remote Receiver, and then Remote Sender.
This ID helps keep track of the responses from the Remote Sender, as the Remote Sender may be running more than one remote program execution simultaneously, and the output from these programs may be interlaced.

The output of the execution on the Remote Sender is sent over the SSH stream back to the Local Receiver with records of the \texttt{Receiver\_query} type serialized to s-expression form:
\begin{lstlisting}
(* Excerpt from rpc_common.ml *)
module Receiver_data = struct
  type t =
    | Message of string
    | Close
    | Heartbeat of int
  [@@deriving sexp, bin_io]
end

module Receiver_query = struct
  type t =
    { id : string
    ; sequence_number : int
    ; data : Receiver_data.t
    }
  [@@deriving sexp, bin_io, fields]
end
\end{lstlisting}

A \texttt{Receiver\_query} record contains a \texttt{data} field of the \texttt{Receiver\_data} variant type, in addition to the ID of the program execution and a sequence number.
The \texttt{Receiver\_data} variant shows the three types of data that can be delivered by the Remote Sender: a message for the standard output, a close notification indicating that execution is complete, and a heartbeat to indicate that the connection is still alive.
When the Local Receiver gets the \texttt{Receiver\_query} record, it uses the RPC server to send this record back to the parent process.

In addition to sending the program to be executed, the Local Sender is also responsible for sending standard input to the remote program, and notifying the remote program when the sender closes the writer writing to the standard input.
The sender query type looks very similar to the receiver query type, except that the Local Sender has no control over the heartbeat and sequence number fields of receiver query, as they originate from the Remote Sender.

\subsection{Heartbeat and sequence number handling}
The Shard protocol exposes an interface for manually processing the \texttt{Receiver\_query} records from the Remote Sender.
This makes sense for achieving greater control over handling the specifics pieces of functionality as provided by the protocol, such as the heartbeat, id, and sequence number, but most users of the protocl can opt to use the \texttt{remote\_cluster} interface that automatically handles reliablility and retries using these features:

\begin{lstlisting}
(* Excerpt from remote_cluster.mli *)
type t

val create : unit -> t

val init_targets
  :  t
  -> targets:Remote_target.t list
  -> stderr:Writer.t
  -> verbose:bool
  -> unit Deferred.Or_error.t

val run_task
  :  t
  -> target:[ `Any | `Specific of Remote_target.t ]
  -> program:Sexp.t
  -> env_image:Env_image.t
  -> send_lines:string list
  -> string Deferred.Or_error.t
\end{lstlisting}

The \texttt{remote\_cluster} interface exposes the abstraction of tasks that need to be completed on a cluster.
When a \texttt{remote\_cluster} is created, the set of possible remote targets need to be initialized with \texttt{init\_targets}, which performs the data transfer setup of getting the remote machine ready for accepting remote commands on each remote target passed in.

When a task is created with \texttt{run\_task}, it is executed on an arbitrary or specific target, depending on what is desired by the caller.
All of the information of the program execution are contained in the \texttt{program}, \texttt{env\_image}, and \texttt{send\_lines} parameter, the first two of which are sent as a \texttt{Header.t} record to start the remote computation, and the third parameter is the lines provided to the standard input of the program.
This task is created with a timeout mechanism that will cause the task to be retried, that is based on various conditions: if the first heartbeat is not received after a set amount of time; if further heartbeats are not received after some time; or if the entire processes's execution takes too long.
A retry is also invoked if an error occurs during the task's execution.
In this case, a further error recovery mechanism is to reinitialize the remote target with a new set of processes, which allows the task to recover from Shard crashes.
This retry mechanism can result in the same task being executed multiple times on different machines, until the task comes back with a successful result (a close message).

In addition, the \texttt{remote\_cluster} keeps track of the sequence number of the \texttt{Receiver\_query}s being sent by the Remote Sender, to make sure to processes the queries in the correct order.
It does so by keeping track of the highest sequence number currently seen, storing messages out of sequence in a hash table until they are the next message in sequence, and producing the final output string when the close query is received.
This means that no output is lost, even if the close query reaches the local machine before some standard output message sent earlier by the Remote Sender.
If a query is dropped, the close query will never be in sequence, so the timeout mechanism will time out the execution, and a retry will be invoked.

Using the \texttt{remote\_cluster} abstraction, the caller can be confident that tasks will be continously reattempted until it completes sucessfully (or wait forever if the task always fails).
Even though the many layers of abstraction used result in a more heavyweight protocol, it is a worthwhile tradeoff to ensure that Shard satisfies the core pillars of being reliable and easy to use, and generally works well enough for running ad-hoc programs, where it is less likely that all system resources are being utilized.

\section{Application class implementation}

% \begin{itemize}
%   \item Each application class implementation conforms to a specific interface
%   \item Handles how new jobs are dispatched, how existing jobs are managed, retry, error handling, etc. as needed by the application class
%   \item Ability to plug in support for new application classes
% \end{itemize}

Application classes are implemented in the Shard project as instances of an application class module type, which specifies the following interface that needs to be conformed by each application class backend:

\begin{lstlisting}
(* Excerpt from application_class.ml *)
module type Backend = sig
  val remote_run
    :  cluster_id:string
    -> remote_targets:Remote_target.t list
    -> setting:string
    -> program:Sexp.t
    -> env_image:Env_image.t
    -> verbose:bool
    -> stdin:Reader.t
    -> stdout:Writer.t
    -> stderr:Writer.t
    -> unit Deferred.Or_error.t
end
\end{lstlisting}

This function is called by the Shard evaluator's remote execution evaluation, which supplies the application class with each of the parameters of the \texttt{remote\_run} function as necessary to sucessfully perform the remote execution.
The implementation of this function depends on the specific application class backend, but generally involves calling functions in one of the remote invocation abstractions provided by the Shard protocol implementation, which then performs the actual remote program execution.
Repeated remote execution calls on the same cluster are guaranteed to use the same \texttt{cluster\_id} string, meaning that the backend can partition its inputs on a per-cluster basis, using global state to maintain a lookup for individual cluster data from its ID.

Although there are currently only two application classes implemented in Shard, it is easy to create additional ones by creating modules that follow the specification of the module type, which should contain all of the information needed by the application class to support any unique features.
At the current moment, the project needs to be modified directly to add builtin support for the application class, but it's possible with some minor changes to externally expose all of the controls for which application classes to use, meaning that Shard can be used as a platform to build additional remote capabilities on top of.

\subsection{Single command}

\todoi{Needs work: this section is currently inaccurate at describing how the implementation actually works, because I haven't gotten around to converting the single command application class to using the remote cluster abstraction yet}

The \texttt{single\_command} application class supports functionality for running a command that should be reliably executed on each of the machines in a cluster. The following example \texttt{single\_command} program gets the memory usage of each machine in a cluster named \texttt{csmachines}, and prints it along with the hostname of the machine to a local output:

\begin{lstlisting}
(h=$(hostname); echo -n "$h: "; free | awk "FNR == 2 {print \$3/\$2}") @@ default | tee memory_log.txt
\end{lstlisting}

The implementation of this application class is very simple, and does not require directly using the the Shard protocol RPC queries: it takes the program input, and uses the \texttt{remote\_cluster} abstraction to reliabily execute the program as a task on each machine in the cluster.
The \texttt{remote\_cluster} guarantees reliability and error recovery, allowing the task to be retried on the same machine if it failed.
% \todoi{Functionality for tuning retries to be less aggressive?}

\subsection{Data parallel}

The \texttt{data\_parallel} application class supports MapReduce-style functionality for running intensive data-parallel applications.
The following example \texttt{data\_parallel} program calculates the number of occurrences of each line length in an input file on the cluster named \texttt{testcluster}:

\begin{lstlisting}[language=shard]
cat input.txt | (echo $in | wc -c; echo 1)@@map/testcluster | (paste -sd+ - | bc)@@reduce/testcluster
\end{lstlisting}

% Move this to the Design section
% For example, running the above program with the input text being Hamlet's Soliloquy (``To be or not to be'') from Shakespeare's Hamlet gives the following output:

% \begin{lstlisting}[language=]
% 29,1
% 37,1
% 38,2
% 39,2
% 40,3
% 41,4
% 42,3
% 43,2
% 44,4
% 45,4
% 46,1
% 47,1
% 48,1
% 49,3
% 52,1
% \end{lstlisting}

When the \texttt{data\_parallel} application class backend receives the \texttt{remote\_run} call, it gets two separate calls, one with the ``map'' setting and one with the ``reduce'' setting.
The backend's functionality is based on the setting of the call, and fails if the setting is not either ``map'' or ``reduce'', as these are required by the \texttt{data\_parallel} application class backend.

For the call with the ``map'' setting, the backend splits the input by the line, and creates a task for each line, where the task is to execute the remote execution payload specified by the \texttt{program} argument.
The environment for this execution is a copy of the local environment with an addtional assignment of the variable \texttt{\$in} to the value of the line.
This is sent to an arbitrary remote host for execution, using the \texttt{remote\_cluster} abstraction provided by the protocol to ensure that the task is reliably completed.
\todoi{Needs work: Describe and implement remote host choosing mechanism}
The program is expected to output two lines: the first of which is the key, and the second of which is the value.
This is printed to the local standardout as a key-value pair in a single line, with the key and value separated by a comma.

For the call with the ``reduce'' setting, it expects the input to be in the form of the ``map'' output, where each input's like is a key-value pair separated by a comma.
The backend collects the inputs by the key, and sends all inputs with the same key to the same remote host.
The program's standard input is now each of the values associated with the key, separated on a per-line basis, and this time, the \texttt{\$key} variable is assigned to the value of the key.
The output is expected to be a single line.
Finally, when all of the reduce tasks complete, the output is sorted by key and printed, with each line being a key-value pair, with the key and value separated by a comma.

The specific implementation of the \texttt{data\_parallel} backend does not require in-depth knowledge about how the Shard protocol functions, as that is cleanly concealed by the \texttt{remote\_cluster} abstraction.
This achieves the separation of concerns for the functionality of the application class itself and the Shard remote protocol, which also makes adding new application classes or improving the Shard protocol much easier.

\section{Implementation trade-offs}

As with any real-world system, there are some trade-offs made in this implementation of Shard.

\subsection{Performance}
% \begin{itemize}
%   \item Quantify performance tradeoffs? examples, stats, graphs, etc
% \end{itemize}
\todoi{Needs work: Need to wait for a bit more data collection in the evaluation section before finishing up this section with some data and maybe comparisons}

The most obvious and significant trade-off to this implementation of Shard is the sacrifice of performance in return for ease of use and implementation.

Out of the three core pillars of Shard (distributed, reliable, easy to use), performance does not directly fit in any of these.
At some level, performance matters for the usability, but Shard suffices in reaching its usability goal as long as Shard can complete tasks at a rate reasonable to its users.

The sacrifice to performance is justified by the targetted use-case of Shard.
Performance is of the highest priority in contexts where scalability matters, where ease of use may be less of a necessity, and other existing distributed systems better fit the niche.
On the other hand, consumer computers today are powerful enough that a suboptimal implementation of Shard performance wise does not have as significant of an impact as it might have in the past.

\subsection{Alternative Approaches}
% \begin{itemize}
%   \item Other languages
%   \item Threads vs processes
% \end{itemize}

The most unique choice made in the implementation of the Shard project was the choice to use the OCaml programming language for almost all of the code.
OCaml has be benefit of being a statically typed, performant, garbage collected, high-level programming language, making it a suitable choice for writing safe and correct programs.
The static typing in conjunction with the automatic code generation for serialization and deserialization is especially helpful for keeping track of records and variants passed across the network, since a type incompatibility problem that is only uncovered at runtime can be difficult to debug.
Although not as widely used as some more popular programming languages, especially in computer systems programming, where C and C++ are traditionally dominant, OCaml still has a moderately developed ecosystem, with a handful of diverse and powerful libraries, such as the Jane Street Core and Async libraries used throughout the implementation, and the Angstrom library used in the Shard parser.

SSH was a clear choice for the transport protocol, due to its plethora of features and widespread adoption.
One notable part of SSH is that an SSH server is often already installed on
Alternatives such as directly building on top of TCP can also be viable choices, for even more flexibility at the cost of implementation time and efforts, but may require additional steps or permissions for installation and configuration, losing out on part of the the core pillar of being easy of use.

As mentioned in the \textbf{Shell implementation} section, rather than building a shell implementation from scratch, it is possible to leverage an existing shell implementation and get a much larger feature set.
However, one problem is that this approach sacrifices some flexibility, especially because many existing shell libraries are not written in OCaml, so foreign language bindings would have to be used to connect with the OCaml code used for the remainder of the project.
For the purposes of Shard, it currently sufficies to use its own shell implementation, though future work may include considering substituting it for a fully-fledged existing shell implementation, as not all useful shell features or language semantics are currently available for Shard.
\todoi{Cite examples of existing shells as libraries}

Another tradeoff already mentioned, this time in the \textbf{Data transfer framework} section, is the tradeoff of using processes rather than threads for creating the Local Sender and Local Receiver to be used in the Shard protocol implementation.
Processes are more heavy weight than threads, both in terms of the performance hit when creating new processes, and in terms of memory usage while the process is alive.
Additionally, inter-process communication requires specific support, as compared to inter-thread communication being facilitated by a shared memory space, which is the reasoning behind the heavy use of remote procedure calls in the Shard implementation.
However, using processes also comes with the upside of separation.
If a child processes crashes or hangs, the parent process can easily kill it and spawn another, leveraging the operating system to avoid a resource leak.
On the other hand, a misbehaving thread can result in the whole program to fail, which would be a hit to the reliability of the overall Shard system.
Also, the resource hit of using processes do not have as much of an impact, because the total number of processes created is not very large, scaling linearly with the number of remote hosts, which should remain as a relatively small fixed number through the course of the execution of a Shard program.

\todoi{Maybe bring up the lack of multi-core support for OCaml? (OCaml threads allow concurrency but not parallelism)}

Overall, the Shard implementation is able to meet the requirements of the design, while remaining aligned with the three core pillars of Shard: being distributed, reliable, and easy to use.

\chapter{Evaluation}

\section{Distributed}
\begin{itemize}
  \item Can be run over network
  \item Graph to show running time for X machines in cluster (use CS machines? rent some VMs?)
\end{itemize}

\section{Reliable}

\begin{itemize}
  \item Still works even in error conditions
  \item Graph completion time vs random failure rate for \texttt{single\_command} programs with intermittent network errors
  \item Graph completion time vs random failure rate for \texttt{data\_parallel} programs with both temporarily and permanently failing hosts
\end{itemize}

\section{Easy to use}

\begin{itemize}
  \item Provide examples of sample programs
  \item If time permits, ask other programmers (potentially with less knowledge on distributed systems/programming languages) to try writing programs
  \item Line of code/code size comparisons?
\end{itemize}

\chapter{Conclusion}
\begin{itemize}
  \item Research question
  \item Solution (design, implementation)
  \item Measuring success (three pillars)
  \item Future work
  \item What did this thesis accomplish?
\end{itemize}

%%%%%%%% References %%%%%%%%%%
\bibliographystyle{acm}
\bibliography{references}
%%%%%%%% End References %%%%%%

\end{document}
